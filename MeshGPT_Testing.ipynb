{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-19T01:39:38.155749Z","iopub.status.busy":"2023-12-19T01:39:38.155103Z","iopub.status.idle":"2023-12-19T01:39:44.048426Z","shell.execute_reply":"2023-12-19T01:39:44.047605Z","shell.execute_reply.started":"2023-12-19T01:39:38.155692Z"},"trusted":true},"outputs":[],"source":["import torch\n","import trimesh\n","import numpy as np\n","import os\n","import csv \n","import json\n","import math \n","\n","from meshgpt_pytorch import (\n","    MeshTransformerTrainer,\n","    MeshAutoencoderTrainer,\n","    MeshAutoencoder,\n","    MeshTransformer\n",")\n","\n","def get_3d_data(file_path): \n","    mesh = trimesh.load(file_path, force='mesh')\n","    \n","    # Extract vertices and faces\n","    vertices = mesh.vertices\n","    faces = mesh.faces\n","       \n","    centered_vertices = vertices - np.mean(vertices, axis=0)\n"," \n","    max_abs = np.max(np.abs(centered_vertices))\n","    vertices_normalized = centered_vertices / (max_abs / 0.95)  \n","     \n","    \n","    # Sort vertices in specified order: y, x, z\n","    vertices_sorted_indices = np.lexsort((vertices_normalized[:, 1], vertices_normalized[:, 0], vertices_normalized[:, 2]))\n","    vertices_normalized_sorted = vertices_normalized[vertices_sorted_indices]\n","    \n","    # Convert indices to tuples for creating Look-Up Table (LUT)\n","    tuples_sorted_indices = [tuple([index]) for index in vertices_sorted_indices.tolist()]\n","    \n","    # Create Look-Up Table (LUT)\n","    lut = {old_index[0]: new_index for new_index, old_index in enumerate(tuples_sorted_indices)}\n","    \n","    # Reindex faces using LUT\n","    faces_reindexed = np.vectorize(lut.get, otypes=[int])(faces) \n","    # Sort faces based on their lowest vertex index\n","    faces_sorted = faces_reindexed[np.lexsort(faces_reindexed.T)]\n","    \n","    #print(f\"{file_path} vertices {len(vertices)} faces {len(faces)}\")\n","    \n","    return vertices_normalized_sorted, faces_sorted \n","\n","def augment_mesh_scalar(vertices, scale_factor):\n","    # Apply a scalar factor to XYZ coordinates\n","    transformed_vertices = vertices * scale_factor\n","    return transformed_vertices\n","\n","def generate_scale_factors(num_examples, lower_limit=0.75, upper_limit=1.25): \n","    scale_factors = np.random.uniform(lower_limit, upper_limit, size=num_examples)\n","    return scale_factors\n","\n","def jitter_mesh(vertices, jitter_factor=0.01): \n","    offsets = np.random.uniform(-jitter_factor, jitter_factor, size=vertices.shape)\n"," \n","    jittered_vertices = vertices + offsets \n","    return jittered_vertices \n","\n","def augment_mesh(vertices, scale_factor):\n","    #vertices = jitter_mesh(vertices)\n","    transformed_vertices = vertices * scale_factor\n","    \n","    return transformed_vertices\n"," \n","\n","def load_models(directory, num_examples, variations):\n","    obj_datas = []  \n","    \n","    print(f\"num_examples: {num_examples}\")\n","    for filename in os.listdir(directory):  \n","        if (filename.endswith(\".obj\") or  filename.endswith(\".glb\") or  filename.endswith(\".off\")):\n","            file_path = os.path.join(directory, filename)\n","\n","            scale_factors = generate_scale_factors(variations, 0.7, 0.9) \n","            vertices, faces = get_3d_data(file_path) \n","\n","            for scale_factor in scale_factors: \n","                aug_vertices = augment_mesh(vertices.copy(), scale_factor) \n","                \n","                for _ in range(num_examples):\n","                    obj_data = {\"vertices\": aug_vertices.tolist(), \"faces\":  faces.tolist(), \"texts\": filename[:-4]}\n","                    obj_datas.append(obj_data)   \n","    return obj_datas\n","  \n","\n","def load_json(file,num_examples):\n","    obj_datas = []\n","    with open(file, \"r\") as json_file:\n","        loaded_data = json.load(json_file) \n","        for item in loaded_data:\n","            for _ in range(num_examples):\n","                obj_data = {\"vertices\": torch.tensor(item[\"vertices\"], dtype=torch.float), \"faces\":  torch.tensor(item[\"faces\"], dtype=torch.long),\"texts\": item[\"texts\"] } \n","                obj_datas.append(obj_data)\n","    return obj_datas\n","  \n","                        \n","         "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-19T01:39:44.050017Z","iopub.status.busy":"2023-12-19T01:39:44.049685Z","iopub.status.idle":"2023-12-19T01:39:44.063671Z","shell.execute_reply":"2023-12-19T01:39:44.062641Z","shell.execute_reply.started":"2023-12-19T01:39:44.049992Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader \n","from tqdm import tqdm\n","import numpy as np\n","import torch\n"," \n","class MeshDataset(Dataset): \n","    \n","    def __init__(self, data): \n","        self.data = data\n","        print(f\"Got {len(data)} data\")\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx): \n","        return self.data[idx]\n","    \n","    def embed_texts(self,transformer): \n","        unique_texts = set(item['texts'] for item in self.data)\n"," \n","        text_embeddings = transformer.embed_texts(list(unique_texts))\n","        print(f\"Got text_embeddings: {len(text_embeddings)}\") \n","        text_embedding_dict = dict(zip(unique_texts, text_embeddings))\n"," \n","        for item in self.data:\n","            text_value = item['texts']\n","            item['text_embeds'] = text_embedding_dict.get(text_value, None)\n","            del item['texts']\n"," \n","        \n","    def sample_obj(self):\n","        all_vertices = []\n","        all_faces = []\n","        vertex_offset = 0 \n","\n","\n","        translation_distance = 0.5  # Adjust as needed \n","        vertex_offset = len(all_vertices)\n","        \n","        for r, faces_coordinates in enumerate(self.data):    \n","            if r > 30:\n","                break\n","            for vertex in faces_coordinates[\"vertices\"]: \n","                all_vertices.append(f\"v {vertex[0]+translation_distance * (r / 0.2 - 1)} {vertex[1]} {vertex[2]}\\n\")\n","                #all_vertices.append(f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\")\n","\n","            for face in faces_coordinates[\"faces\"]:\n","                all_faces.append(f\"f {face[0]+1+vertex_offset} {face[1]+1+vertex_offset} {face[2]+1+vertex_offset}\\n\") \n","                try:\n","                    all_vertices[face[0]+vertex_offset]\n","                    all_vertices[face[1]+vertex_offset]\n","                    all_vertices[face[2]+vertex_offset]\n","                except Exception  as e :\n","                    print(e)\n","                    print(face[0]+vertex_offset)\n","                    print(face[1]+vertex_offset)\n","                    print(face[2]+vertex_offset)\n","                    print(len(all_vertices))\n","                \n","            vertex_offset = len(all_vertices) \n","\n","\n","        obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n","\n","        # Save to a single file\n","        obj_file_path = \"./combined_3d_models.obj\"\n","        with open(obj_file_path, \"w\") as file:\n","            file.write(obj_file_content)\n","\n","        print(obj_file_path)\n","         "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-19T01:39:44.066146Z","iopub.status.busy":"2023-12-19T01:39:44.065812Z","iopub.status.idle":"2023-12-19T01:39:52.313349Z","shell.execute_reply":"2023-12-19T01:39:52.312381Z","shell.execute_reply.started":"2023-12-19T01:39:44.066116Z"},"trusted":true},"outputs":[],"source":["import json\n","#tables = load_models(r\" filtered\",5,5)  \n","#with open(\"data.json\", \"w\") as json_file:\n","#    json.dump(tables, json_file)\n","\n","\n","tables = load_json(\"/kaggle/input/shapenet/data.json\",2)\n","dataset = MeshDataset(tables) \n","\n","unique_values = set(item[\"texts\"] for item in dataset.data)\n","\n","print(len(unique_values))  \n","print(unique_values)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["autoencoder = MeshAutoencoder( \n","    num_discrete_coors = 128  , \n",") \n","total_params = sum(p.numel() for p in autoencoder.encoders.parameters())\n","print(f\"encoders Total parameters: {total_params}\")\n","total_params = sum(p.numel() for p in autoencoder.decoders.parameters())\n","print(f\"decoders Total parameters: {total_params}\")  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-19T01:44:00.715656Z","iopub.status.busy":"2023-12-19T01:44:00.714727Z","iopub.status.idle":"2023-12-19T12:00:40.840982Z","shell.execute_reply":"2023-12-19T12:00:40.840110Z","shell.execute_reply.started":"2023-12-19T01:44:00.715620Z"},"trusted":true},"outputs":[],"source":["total_params = sum(p.numel() for p in autoencoder.encoders.parameters())\n","print(f\"Total parameters: {total_params}\")\n","print(autoencoder.encoders)\n","\n","autoencoder_trainer = MeshAutoencoderTrainer(model =autoencoder,learning_rate = 1e-3, \n","                                             warmup_steps = 10,\n","                                             dataset = dataset,   \n","                                             num_train_steps=100,\n","                                             batch_size=16,\n","                                             grad_accum_every=1)\n","\n","loss = autoencoder_trainer.train(40,stop_at_loss = 0.25)   \n","autoencoder_trainer = MeshAutoencoderTrainer(model =autoencoder,learning_rate = 1e-4, \n","                                             warmup_steps = 10,\n","                                             dataset = dataset,\n","                                             checkpoint_every_epoch = 20,  \n","                                             num_train_steps=100,\n","                                             batch_size=16,\n","                                             grad_accum_every=1)\n","\n","loss = autoencoder_trainer.train(180,stop_at_loss = 0.25)   \n","autoencoder_trainer.save(f'./mesh-encoder_2_loss_{loss:.3f}.pt') "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["max_length =  max(len(d[\"faces\"]) for d in dataset if \"faces\" in d) \n","max_seq =  max_length * 6  \n","print(max_length)\n","print(max_seq)\n","transformer = MeshTransformer(\n","    autoencoder,\n","    dim = 768,\n","    max_seq_len = max_seq,\n","    condition_on_text = True\n",")\n","total_params = sum(p.numel() for p in transformer.parameters())\n","print(f\"Total parameters: {total_params}\") "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Untested, but should work (?)\n","\n","#dataset.embed_texts(transformer)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[" \n","trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=1,num_train_steps=100,   dataset = dataset,\n","                                 learning_rate = 1e-1, batch_size=2)\n","trainer.train(80,stop_at_loss = 0.00009)   \n","\n"," \n","trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=1,num_train_steps=100,  dataset = dataset,\n","                                 learning_rate = 1e-2, batch_size=2)\n","trainer.train(80,stop_at_loss = 0.00009)    \n","\n","trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=1,num_train_steps=100,  dataset = dataset,\n","                                 learning_rate = 1e-4, batch_size=2)\n","trainer.train(80,stop_at_loss = 0.00009)   \n","\n","trainer.save(f'./mesh-transformer_2_{loss:.3f}.pt')    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["unique_values = set(item[\"texts\"] for item in dataset.data)\n","print(len(unique_values))  \n","coords = []\n","for text in unique_values: \n","    print(f\"doing {text}\")\n","    faces_coordinates = transformer.generate(texts = [text]) \n","    coords.append(faces_coordinates)\n","    tensor_data = faces_coordinates[0].cpu()\n","    \n","    numpy_data = tensor_data.numpy().reshape(-1, 3)\n","    \n","    obj_file_content = \"\"\n","    \n","    for vertex in numpy_data:\n","        obj_file_content += f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\"\n","\n","    for i in range(1, len(numpy_data), 3):\n","        obj_file_content += f\"f {i} {i + 1} {i + 2}\\n\"\n","\n","    # Save to a file\n","    obj_file_path = f'./tests/3d_output_{text}.obj'\n","    with open(obj_file_path, \"w\") as file:\n","        file.write(obj_file_content)\n","\n","    print(obj_file_path) \n","    \n","    \n","all_vertices = []\n","all_faces = []\n","vertex_offset = 0\n"," \n","translation_distance = 0.3  \n","\n","for r, faces_coordinates in enumerate(coords): \n","    tensor_data = faces_coordinates[0].cpu()\n","\n","    numpy_data = tensor_data.numpy().reshape(-1, 3)\n","\n","    # Translate the model to avoid overlapping\n","    numpy_data[:, 0] += translation_distance * (r / 0.2 - 1)  # Adjust X coordinate\n","\n","    # Accumulate vertices\n","    for vertex in numpy_data:\n","        all_vertices.append(f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\")\n","\n","    # Accumulate faces with adjusted indices\n","    for i in range(1, len(numpy_data), 3):\n","        all_faces.append(f\"f {i + vertex_offset} {i + 1 + vertex_offset} {i + 2 + vertex_offset}\\n\")\n","\n","    # Update the vertex offset for the next model\n","    vertex_offset += len(numpy_data)\n","\n","# Combine vertices and faces\n","obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n","\n","# Save to a single file\n","obj_file_path = f\"./tests/3d_models_all.obj\"\n","with open(obj_file_path, \"w\") as file:\n","    file.write(obj_file_content)\n","\n","print(obj_file_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","coords_all = []\n","for text in set(item[\"texts\"] for item in dataset.data): \n","    print(f\"Doing {text}\")\n","    coords = []\n","    for r in np.arange(0, 1.0, 0.1):\n","        faces_coordinates = transformer.generate(temperature=r, texts = [text]) \n","        coords.append(faces_coordinates)\n","    coords_all.append(coords)\n","    \n","    all_vertices = []\n","    all_faces = []\n","    vertex_offset = 0\n","\n","    # Translation distance for each model\n","    translation_distance = 0.3  # Adjust as needed\n","\n","    for r, faces_coordinates in enumerate(coords): \n","        tensor_data = faces_coordinates[0].cpu()\n","\n","        numpy_data = tensor_data.numpy().reshape(-1, 3)\n","\n","        # Translate the model to avoid overlapping\n","        numpy_data[:, 0] += translation_distance * (r / 0.2 - 1)  # Adjust X coordinate\n","\n","        # Accumulate vertices\n","        for vertex in numpy_data:\n","            all_vertices.append(f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\")\n","\n","        # Accumulate faces with adjusted indices\n","        for i in range(1, len(numpy_data), 3):\n","            all_faces.append(f\"f {i + vertex_offset} {i + 1 + vertex_offset} {i + 2 + vertex_offset}\\n\")\n","\n","        # Update the vertex offset for the next model\n","        vertex_offset += len(numpy_data)\n","\n","    # Combine vertices and faces\n","    obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n","\n","    # Save to a single file\n","    obj_file_path = f\"./results/3d_models_{text}_temps.obj\"\n","    with open(obj_file_path, \"w\") as file:\n","        file.write(obj_file_content)\n","\n","    print(obj_file_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def loadModels():\n","    autoencoder = MeshAutoencoder(\n","        dim = 576,\n","        encoder_depth = 6,\n","        decoder_depth = 6,\n","        num_discrete_coors = 128  ,\n","        local_attn_depth =0, \n","        \n","    )\n","    autoencoder_trainer = MeshAutoencoderTrainer(model = autoencoder,\n","                                    learning_rate = 1e-1, \n","                                                checkpoint_every_epoch= 5,\n","                                                warmup_steps = 10,\n","                                                dataset = dataset,  \n","                                                num_train_steps=100,\n","                                                batch_size=2,\n","                                                grad_accum_every=1)\n","\n","    autoencoder_trainer.load(r\"mesh-encoder_last.pt\")\n","    encoder = autoencoder_trainer.model\n","    max_length =  max(len(d[\"faces\"]) for d in dataset if \"faces\" in d) \n","    max_seq =  max_length * 6  \n","    \n","    transformer = MeshTransformer(\n","        autoencoder,\n","        dim = 768,\n","        max_seq_len = max_seq,\n","        condition_on_text = True)\n","     \n","    trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=1,num_train_steps=100, checkpoint_folder = r\"F:\\MachineLearning\\Mesh\\MeshGPT\\checkpoints\" , dataset = dataset,\n","                                    learning_rate = 1e-3, batch_size=2) \n","    trainer.load(r\"mesh-transformer.pt\")\n","    transformer = trainer.model\n","    return transformer, encoder\n","\n","#transformer, autoencoder =  loadModels() "]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4189032,"sourceId":7234116,"sourceType":"datasetVersion"}],"dockerImageVersionId":30627,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
